{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torchaudio\\extension\\extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torchaudio\\backend\\utils.py:64: UserWarning: The interface of \"soundfile\" backend is planned to change in 0.8.0 to match that of \"sox_io\" backend and the current interface will be removed in 0.9.0. To use the new interface, do `torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE = False` before setting the backend to \"soundfile\". Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  'The interface of \"soundfile\" backend is planned to change in 0.8.0 to '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('C:\\\\Users\\\\Admin\\\\Downloads\\\\combine_all_v2.csv', mode='r', encoding='utf-8', newline='') as file:\n",
    "    reader = csv.reader(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    data = [line for line in reader]\n",
    "    \n",
    "head = data[0]\n",
    "train_data, test_data = train_test_split(data[1:], train_size = 0.8)\n",
    "with open('train.csv', mode='w', encoding='utf-8', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(head)\n",
    "    for line in train_data:\n",
    "        writer.writerow(line)\n",
    "        \n",
    "with open('test.csv', mode='w', encoding='utf-8', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(head)\n",
    "    for line in test_data:\n",
    "        writer.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86425, 9)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "print(data.shape)\n",
    "user_ids = data.user_id.unique()\n",
    "song_ids = data.song_id.unique()\n",
    "num_users = len(user_ids)\n",
    "num_songs = len(song_ids)\n",
    "user_ids_to_idx = dict(zip(user_ids, range(num_users)))\n",
    "song_ids_to_idx = dict(zip(song_ids, range(num_songs)))\n",
    "\n",
    "Y = np.zeros((num_users, num_songs))\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    user_idx = user_ids_to_idx[data.user_id[i]]\n",
    "    song_idx = song_ids_to_idx[data.song_id[i]]\n",
    "    Y[user_idx][song_idx] = int(data.listen_count[i])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53196, 9244)\n",
      "(9244, 53196)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(Y.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "19\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "print(Y[[1,3,5],[2,4,6]])\n",
    "print(99//5)\n",
    "print(19*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSys(nn.Module):\n",
    "    def __init__(self, num_users, num_songs, embedding_size):\n",
    "        super().__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_songs = num_songs\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_encode = nn.Sequential(\n",
    "            nn.Linear(num_songs, embedding_size, bias=False),\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size)\n",
    "        )\n",
    "        self.song_encode = nn.Sequential(\n",
    "            nn.Linear(num_users, embedding_size, bias=False),\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size)\n",
    "        )\n",
    "        self.cosine = nn.CosineSimilarity()\n",
    "\n",
    "    def forward(self, user, song):\n",
    "        user = torch.tensor(user).cuda().float()\n",
    "        song = torch.tensor(song).cuda().float()\n",
    "        p = self.user_encode(user)\n",
    "        q = self.song_encode(song)\n",
    "        out = self.cosine(p, q)\n",
    "        return p, q, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "tensor([-0.0203, -0.0111], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = RecSys(num_users=num_users, num_songs=num_songs, embedding_size=256).cuda().float()\n",
    "p, q, out = model(Y[:2],Y.T[:2])\n",
    "print(p.shape,q.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b76a67934326>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-b76a67934326>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, Y, num_epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0msong_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv_j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msong_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-e41e543c5f11>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, user, song)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0muser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0msong\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msong_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = RecSys(num_users=num_users, num_songs=num_songs, embedding_size=512).cuda().float()\n",
    "\n",
    "def train(model, Y, num_epochs=100, batch_size=128, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    num_batch_user = num_users//batch_size\n",
    "    num_batch_song = num_songs//batch_size\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        user_idx = list(range(num_users))\n",
    "        np.random.shuffle(user_idx)\n",
    "        song_idx = list(range(num_songs))\n",
    "        np.random.shuffle(song_idx)\n",
    "        c1 = time.time()\n",
    "        for i in range(num_batch_user):\n",
    "            if (i+1)*batch_size > num_users:\n",
    "                u_i = user_idx[i*batch_size: num_users]\n",
    "            else:\n",
    "                u_i = user_idx[i*batch_size: (i+1)*batch_size]\n",
    "            \n",
    "            user_batch = Y[u_i]\n",
    "            for j in range(num_batch_song):    \n",
    "                if (j+1)*batch_size > num_songs:\n",
    "                    v_j = song_idx[j*batch_size: num_songs]\n",
    "                else:\n",
    "                    v_j = song_idx[j*batch_size: (j+1)*batch_size]\n",
    "                    \n",
    "                song_batch = Y.T[v_j]\n",
    "                \n",
    "                p, q, out = model(user_batch, song_batch)\n",
    "                target = torch.tensor(Y[u_i, v_j]).cuda().float()\n",
    "                loss = criterion(out, target)\n",
    "                losses.append(loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        c2 = time.time()\n",
    "        print('epoch: {}, time: {}s, loss: {}'.format(epoch, c2-c1, sum(losses)/len(losses)))\n",
    "        losses = []\n",
    "        \n",
    "                \n",
    "train(model, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import csv\n",
    "import eyed3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "title = pd.read_csv(\"C:\\\\Users\\\\Admin\\\\Downloads\\\\combine_all_v2.csv\").title.unique()\n",
    "directory = 'C:/Users/Admin/Downloads/fma_medium'\n",
    "full = listdir(directory)\n",
    "d = []\n",
    "\n",
    "for i in full:\n",
    "    if isdir(join(directory, i)):\n",
    "        d.append(i)\n",
    "        \n",
    "for direc in d:\n",
    "    file = listdir(join(directory, direc))\n",
    "    for f in file:\n",
    "        audio = eyed3.load(join(directory, direc, f))\n",
    "        t = audio.tag.title\n",
    "        if t in title:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load('test.wav')\n",
    "print(waveform.shape)\n",
    "print(sample_rate)\n",
    "plt.figure()\n",
    "plt.plot(waveform.t().numpy())\n",
    "specgram = torchaudio.transforms.Spectrogram()(waveform)\n",
    "\n",
    "print(\"Shape of spectrogram: {}\".format(specgram.size()))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(specgram.log2()[0,:,:].numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "url = \"https://pytorch.org/tutorials/_static/img/steam-train-whistle-daniel_simon-converted-from-mp3.wav\"\n",
    "r = requests.get(url)\n",
    "\n",
    "with open('steam-train-whistle-daniel_simon-converted-from-mp3.wav', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "filename = \"steam-train-whistle-daniel_simon-converted-from-mp3.wav\"\n",
    "waveform, sample_rate = torchaudio.load(filename)\n",
    "\n",
    "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
    "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(waveform.t().numpy())\n",
    "\n",
    "new_sample_rate = 8000\n",
    "transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)\n",
    "transformed = transform(waveform)\n",
    "\n",
    "ipd.Audio(transformed.numpy(), rate=new_sample_rate)\n",
    "plt.figure()\n",
    "plt.plot(transformed.t().numpy())\n",
    "print(transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
